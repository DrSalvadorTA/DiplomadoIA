<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arquitectura Transformer y LLMs - Módulo 2</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">
    <style>
        body {
            margin: 0;
            padding: 0;
            font-family: 'Inter', sans-serif;
            background: linear-gradient(135deg, #f0f4f8 0%, #e2e8f0 100%);
            scroll-behavior: smooth;
        }
        
        .gradient-bg {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        
        .content-card {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 25px;
            box-shadow: 0 25px 50px rgba(0, 0, 0, 0.15);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
            transition: all 0.3s ease;
        }
        
        .card-hover {
            transition: all 0.3s ease;
            transform: translateY(0);
        }
        
        .card-hover:hover {
            transform: translateY(-8px);
            box-shadow: 0 30px 60px rgba(0, 0, 0, 0.2);
        }
        
        .nav-button {
            background: linear-gradient(135deg, #4299e1 0%, #2b6cb0 100%);
            color: white;
            padding: 15px 30px;
            border-radius: 15px;
            text-decoration: none;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            transition: all 0.3s ease;
            box-shadow: 0 8px 25px rgba(66, 153, 225, 0.3);
            font-size: 1.1rem;
        }
        
        .nav-button:hover {
            transform: translateY(-3px);
            box-shadow: 0 15px 35px rgba(66, 153, 225, 0.4);
            text-decoration: none;
            color: white;
        }
        
        .nav-button i {
            margin-right: 10px;
        }
        
        .accent-line {
            height: 4px;
            background: linear-gradient(90deg, #4299e1, #667eea);
            border-radius: 2px;
            width: 120px;
            margin: 0 auto 35px auto;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
            border-left: 6px solid #4299e1;
            padding: 25px;
            border-radius: 12px;
            margin: 25px 0;
            box-shadow: 0 4px 15px rgba(66, 153, 225, 0.1);
        }
        
        .icon-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin-top: 25px;
        }
        
        .icon-card {
            background: rgba(255, 255, 255, 0.9);
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            transition: all 0.3s ease;
            border: 1px solid rgba(66, 153, 225, 0.1);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.08);
        }
        
        .icon-card:hover {
            transform: translateY(-8px);
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
        }
        
        .attention-pattern {
            background: linear-gradient(45deg, #FEF3C7, #FDE68A, #FCD34D);
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.8; }
        }
        
        .decorative-shape {
            position: absolute;
            background: linear-gradient(135deg, #4299e1 0%, #667eea 100%);
            opacity: 0.08;
            border-radius: 50%;
            z-index: -1;
        }
        
        .shape-top-left {
            width: 300px;
            height: 300px;
            top: -150px;
            left: -150px;
        }
        
        .shape-bottom-right {
            width: 250px;
            height: 250px;
            bottom: -125px;
            right: -125px;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 12px;
            overflow: hidden;
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
        }
        
        .comparison-table th,
        .comparison-table td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid rgba(66, 153, 225, 0.1);
        }
        
        .comparison-table th {
            background: linear-gradient(135deg, #4299e1 0%, #667eea 100%);
            color: white;
            font-weight: 600;
        }
        
        .module-icon {
            background: linear-gradient(135deg, #4299e1 0%, #667eea 100%);
            color: white;
            width: 100px;
            height: 100px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 15px 40px rgba(66, 153, 225, 0.4);
        }
        
        .slide-title-main {
            font-size: 4rem;
            font-weight: bold;
            color: #1e3a8a;
            text-align: center;
            margin-bottom: 1.5rem;
            text-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            line-height: 1.1;
        }
        
        .slide-subtitle {
            font-size: 1.8rem;
            color: #667eea;
            text-align: center;
            margin-bottom: 0.8rem;
            font-weight: 600;
        }
        
        .slide-content-title {
            font-size: 2.8rem;
            font-weight: bold;
            color: #1e40af;
            margin-bottom: 1.5rem;
            text-align: center;
            text-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        @media (max-width: 768px) {
            .slide-title-main {
                font-size: 2.5rem;
            }
            
            .slide-content-title {
                font-size: 2rem;
            }
            
            .content-card {
                padding: 25px;
            }
        }
    </style>
</head>
<body class="font-sans">
    <!-- Header con navegación -->
    <header class="gradient-bg text-white py-16 px-4 sm:px-6 lg:px-8 relative overflow-hidden">
        <div class="decorative-shape shape-top-left"></div>
        <div class="decorative-shape shape-bottom-right"></div>
        
        <div class="max-w-6xl mx-auto text-center relative z-10">
            <div class="module-icon mx-auto">
                <i class="fas fa-brain"></i>
            </div>
            <h1 class="slide-title-main text-white mb-6">Arquitectura Transformer</h1>
            <p class="slide-subtitle text-white opacity-90 mb-8">La revolución que cambió el procesamiento de lenguaje natural y dio origen a los modelos de lenguaje grandes (LLMs)</p>
            
            <!-- Botón de navegación al inicio -->
            <div class="mt-8">
                <a href="modulo2.html" class="nav-button">
                    <i class="fas fa-arrow-left"></i>
                    Volver al Módulo 2
                </a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="max-w-7xl mx-auto py-12 px-4 sm:px-6 lg:px-8">
        <!-- Intro Section -->
        <section class="mb-16">
            <div class="text-center max-w-4xl mx-auto">
                <h2 class="slide-content-title">¿Qué es la Arquitectura Transformer?</h2>
                <div class="accent-line"></div>
                <p class="text-xl text-gray-600 mb-8 leading-relaxed">
                    Los Transformers son una arquitectura de red neuronal introducida en 2017 que revolucionó el procesamiento de lenguaje natural. Su principal innovación es el mecanismo de atención que permite procesar secuencias completas de manera paralela, sin necesidad de recursión.
                </p>
                <div class="highlight-box">
                    <p class="text-primary font-semibold text-lg">
                        <i class="fas fa-quote-left mr-2"></i>
                        "Attention is All You Need" - Vaswani et al., 2017
                        <i class="fas fa-quote-right ml-2"></i>
                    </p>
                </div>
            </div>
        </section>

        <!-- Conceptos Fundamentales -->
        <section class="mb-16">
            <h2 class="slide-content-title">Conceptos Fundamentales de Transformers</h2>
            <div class="accent-line"></div>
            
            <!-- Mecanismo de Atención -->
            <div class="content-card card-hover mb-8">
                <div class="grid grid-cols-1 lg:grid-cols-2 gap-8 items-center">
                    <div>
                        <h3 class="text-2xl font-bold mb-4 text-primary flex items-center">
                            <i class="fas fa-eye mr-3 text-2xl"></i>
                            Mecanismo de Atención
                        </h3>
                        <p class="text-gray-600 mb-6 text-lg leading-relaxed">
                            El corazón de los Transformers. Permite al modelo "prestar atención" a diferentes partes de la secuencia de entrada simultáneamente, capturando relaciones de largo alcance entre palabras.
                        </p>
                        <div class="highlight-box">
                            <p class="text-sm text-red-800">
                                <strong>Fórmula de atención:</strong> Attention(Q,K,V) = softmax(QK^T/√d_k)V
                                <br>Donde Q=Query, K=Key, V=Value son transformaciones lineales de la entrada.
                            </p>
                        </div>
                    </div>
                    <div class="attention-pattern p-8 rounded-lg">
                        <div class="text-center">
                            <div class="w-20 h-20 bg-primary rounded-full flex items-center justify-center mx-auto mb-6">
                                <i class="fas fa-bolt text-white text-2xl"></i>
                            </div>
                            <h4 class="font-bold text-gray-800 text-xl mb-2">Self-Attention</h4>
                            <p class="text-gray-600">Cada palabra puede "ver" todas las demás</p>
                        </div>
                    </div>
                </div>
                
                <!-- Video explicativo de Self-Attention -->
                <div class="mt-8">
                    <h4 class="text-xl font-bold mb-4 text-center text-primary">
                        <i class="fas fa-play-circle mr-2"></i>
                        Video: Explicación Visual de Self-Attention
                    </h4>
                    <div class="relative w-full max-w-4xl mx-auto">
                        <div class="aspect-w-16 aspect-h-9">
                            <iframe 
                                class="w-full h-96 rounded-lg shadow-lg"
                                src="https://www.youtube.com/embed/awGfhmsN7Lc" 
                                title="Explicación de Self-Attention en Transformers"
                                frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen>
                            </iframe>
                        </div>
                        <div class="mt-4 text-center">
                            <p class="text-sm text-gray-600">
                                <i class="fas fa-info-circle mr-1"></i>
                                Este video explica de manera visual cómo funciona el mecanismo de Self-Attention en los Transformers
                            </p>
                        </div>
                    </div>
                </div>
                </div>
            </div>

            <!-- Multi-Head Attention -->
            <div class="content-card card-hover mb-8">
                <h3 class="text-2xl font-bold mb-8 text-center text-primary">Multi-Head Attention</h3>
                <div class="grid grid-cols-1 md:grid-cols-4 gap-6">
                    <div class="icon-card">
                        <div class="bg-blue-100 w-20 h-20 rounded-full flex items-center justify-center mx-auto mb-4">
                            <span class="text-2xl font-bold text-blue-600">H1</span>
                        </div>
                        <h4 class="font-bold text-gray-800 mb-2">Head 1</h4>
                        <p class="text-sm text-gray-600">Se enfoca en relaciones sintácticas</p>
                    </div>
                    <div class="icon-card">
                        <div class="bg-green-100 w-20 h-20 rounded-full flex items-center justify-center mx-auto mb-4">
                            <span class="text-2xl font-bold text-green-600">H2</span>
                        </div>
                        <h4 class="font-bold text-gray-800 mb-2">Head 2</h4>
                        <p class="text-sm text-gray-600">Captura relaciones semánticas</p>
                    </div>
                    <div class="icon-card">
                        <div class="bg-yellow-100 w-20 h-20 rounded-full flex items-center justify-center mx-auto mb-4">
                            <span class="text-2xl font-bold text-yellow-600">H3</span>
                        </div>
                        <h4 class="font-bold text-gray-800 mb-2">Head 3</h4>
                        <p class="text-sm text-gray-600">Identifica entidades y co-referencias</p>
                    </div>
                    <div class="icon-card">
                        <div class="bg-purple-100 w-20 h-20 rounded-full flex items-center justify-center mx-auto mb-4">
                            <span class="text-2xl font-bold text-purple-600">H...</span>
                        </div>
                        <h4 class="font-bold text-gray-800 mb-2">Head N</h4>
                        <p class="text-sm text-gray-600">Patrones específicos del dominio</p>
                    </div>
                </div>
                <div class="highlight-box mt-6">
                    <p class="text-orange-800">
                        <strong>Ventaja clave:</strong> Múltiples "cabezas" de atención operan en paralelo, cada una aprendiendo diferentes tipos de relaciones en los datos. GPT-4 tiene hasta 128 cabezas de atención por capa.
                    </p>
                </div>
            </div>

            <!-- Conceptos Clave Grid -->
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6 mb-8">
                <!-- Embeddings Posicionales -->
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-blue-100 p-3 rounded-full mr-4">
                            <i class="fas fa-map-marker-alt text-blue-600 text-xl"></i>
                        </div>
                        <h4 class="text-lg font-bold text-gray-800">Embeddings Posicionales</h4>
                    </div>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Como los Transformers procesan todas las posiciones en paralelo, necesitan información explícita sobre el orden de las palabras mediante encodings posicionales.
                    </p>
                </div>

                <!-- Feed-Forward Networks -->
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-green-100 p-3 rounded-full mr-4">
                            <i class="fas fa-network-wired text-green-600 text-xl"></i>
                        </div>
                        <h4 class="text-lg font-bold text-gray-800">Redes Feed-Forward</h4>
                    </div>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Después de la atención, cada posición pasa por una red neuronal densa independiente que procesa la información contextual capturada.
                    </p>
                </div>

                <!-- Layer Normalization -->
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-purple-100 p-3 rounded-full mr-4">
                            <i class="fas fa-chart-line text-purple-600 text-xl"></i>
                        </div>
                        <h4 class="text-lg font-bold text-gray-800">Layer Normalization</h4>
                    </div>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Técnica de normalización que estabiliza el entrenamiento de redes profundas, aplicada antes de cada sub-capa en los Transformers.
                    </p>
                </div>

                <!-- Conexiones Residuales -->
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-yellow-100 p-3 rounded-full mr-4">
                            <i class="fas fa-exchange-alt text-yellow-600 text-xl"></i>
                        </div>
                        <h4 class="text-lg font-bold text-gray-800">Conexiones Residuales</h4>
                    </div>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Permiten que la información fluya directamente a través de las capas, facilitando el entrenamiento de redes muy profundas y evitando el problema del desvanecimiento del gradiente.
                    </p>
                </div>

                <!-- Paralelización -->
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-red-100 p-3 rounded-full mr-4">
                            <i class="fas fa-tachometer-alt text-red-600 text-xl"></i>
                        </div>
                        <h4 class="text-lg font-bold text-gray-800">Paralelización Masiva</h4>
                    </div>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        A diferencia de las RNN, los Transformers procesan toda la secuencia simultáneamente, permitiendo entrenamiento más rápido en hardware paralelo como GPUs.
                    </p>
                </div>

                <!-- Escalabilidad -->
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-indigo-100 p-3 rounded-full mr-4">
                            <i class="fas fa-expand-arrows-alt text-indigo-600 text-xl"></i>
                        </div>
                        <h4 class="text-lg font-bold text-gray-800">Escalabilidad Extrema</h4>
                    </div>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Los Transformers escalan eficientemente con más parámetros y datos. GPT-4 tiene más de 1 billón de parámetros distribuidos en cientos de capas.
                    </p>
                </div>
            </div>

            <!-- Encoder vs Decoder -->
            <div class="content-card card-hover bg-white">
                <h3 class="text-2xl font-bold mb-8 text-center text-gray-800">Arquitecturas: Encoder vs Decoder</h3>
                <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
                    <div class="bg-blue-50 p-6 rounded-lg border-l-4 border-blue-500">
                        <h4 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                            <i class="fas fa-lock mr-3 text-blue-600"></i>
                            Encoder-Only (BERT)
                        </h4>
                        <ul class="space-y-2 text-sm text-gray-700">
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Procesa secuencias bidireccionales</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Ideal para comprensión de texto</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Tareas: clasificación, Q&A</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Ve todo el contexto simultáneamente</li>
                        </ul>
                    </div>
                    <div class="bg-green-50 p-6 rounded-lg border-l-4 border-green-500">
                        <h4 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                            <i class="fas fa-pen-fancy mr-3 text-green-600"></i>
                            Decoder-Only (GPT)
                        </h4>
                        <ul class="space-y-2 text-sm text-gray-700">
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Genera texto de forma autoregresiva</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Enmascaramiento causal (solo ve el pasado)</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Tareas: generación de texto, completado</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Base de ChatGPT y modelos conversacionales</li>
                        </ul>
                    </div>
                    <div class="bg-purple-50 p-6 rounded-lg border-l-4 border-purple-500">
                        <h4 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                            <i class="fas fa-exchange-alt mr-3 text-purple-600"></i>
                            Encoder-Decoder (T5)
                        </h4>
                        <ul class="space-y-2 text-sm text-gray-700">
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Combinación de ambos componentes</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Ideal para traducción automática</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Tareas: resumen, traducción</li>
                            <li class="flex items-center"><i class="fas fa-check mr-2 text-green-600"></i>Encoder ve todo, Decoder genera secuencialmente</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- LLMs Section -->
        <section class="mb-16">
            <h2 class="slide-content-title">Modelos de Lenguaje Grandes (LLMs)</h2>
            <div class="accent-line"></div>
            
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                <!-- GPT Family -->
                <div class="icon-card card-hover">
                    <div class="bg-green-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-comments text-green-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-3 text-center">Familia GPT (OpenAI)</h3>
                    <p class="text-gray-600 mb-4 text-sm leading-relaxed">
                        Modelos decoder-only entrenados para predecir la siguiente palabra. GPT-4 puede razonar, crear código, y mantener conversaciones complejas.
                    </p>
                    <div class="mt-4 text-center">
                        <span class="inline-block bg-green-100 text-green-800 text-xs px-2 py-1 rounded-full mr-2">175B+ parámetros</span>
                        <span class="inline-block bg-green-100 text-green-800 text-xs px-2 py-1 rounded-full">Conversacional</span>
                    </div>
                </div>

                <!-- BERT Family -->
                <div class="icon-card card-hover">
                    <div class="bg-blue-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-search text-blue-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-3 text-center">Familia BERT (Google)</h3>
                    <p class="text-gray-600 mb-4 text-sm leading-relaxed">
                        Modelos encoder-only bidireccionales, excelentes para comprensión de texto, clasificación y extracción de información.
                    </p>
                    <div class="mt-4 text-center">
                        <span class="inline-block bg-blue-100 text-blue-800 text-xs px-2 py-1 rounded-full mr-2">110M-340M parámetros</span>
                        <span class="inline-block bg-blue-100 text-blue-800 text-xs px-2 py-1 rounded-full">Comprensión</span>
                    </div>
                </div>

                <!-- T5 -->
                <div class="icon-card card-hover">
                    <div class="bg-purple-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-sync-alt text-purple-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-3 text-center">T5 (Google)</h3>
                    <p class="text-gray-600 mb-4 text-sm leading-relaxed">
                        "Text-to-Text Transfer Transformer" convierte todas las tareas de NLP a formato texto-a-texto usando arquitectura encoder-decoder.
                    </p>
                    <div class="mt-4 text-center">
                        <span class="inline-block bg-purple-100 text-purple-800 text-xs px-2 py-1 rounded-full mr-2">220M-11B parámetros</span>
                        <span class="inline-block bg-purple-100 text-purple-800 text-xs px-2 py-1 rounded-full">Multi-tarea</span>
                    </div>
                </div>

                <!-- Claude -->
                <div class="icon-card card-hover">
                    <div class="bg-orange-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-shield-alt text-orange-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-3 text-center">Claude (Anthropic)</h3>
                    <p class="text-gray-600 mb-4 text-sm leading-relaxed">
                        Modelo entrenado con "Constitutional AI" para ser más útil, inofensivo y honesto. Enfoque en seguridad y alineación.
                    </p>
                    <div class="mt-4 text-center">
                        <span class="inline-block bg-orange-100 text-orange-800 text-xs px-2 py-1 rounded-full mr-2">Parámetros no revelados</span>
                        <span class="inline-block bg-orange-100 text-orange-800 text-xs px-2 py-1 rounded-full">Seguridad</span>
                    </div>
                </div>

                <!-- LLaMA -->
                <div class="icon-card card-hover">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-code text-red-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-3 text-center">LLaMA (Meta)</h3>
                    <p class="text-gray-600 mb-4 text-sm leading-relaxed">
                        Modelos de investigación que han democratizado el acceso a LLMs potentes, especialmente LLaMA 2 con licencia comercial.
                    </p>
                    <div class="mt-4 text-center">
                        <span class="inline-block bg-red-100 text-red-800 text-xs px-2 py-1 rounded-full mr-2">7B-65B parámetros</span>
                        <span class="inline-block bg-red-100 text-red-800 text-xs px-2 py-1 rounded-full">Código abierto</span>
                    </div>
                </div>

                <!-- Gemini -->
                <div class="icon-card card-hover">
                    <div class="bg-indigo-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-star text-indigo-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-3 text-center">Gemini (Google)</h3>
                    <p class="text-gray-600 mb-4 text-sm leading-relaxed">
                        Modelo multimodal nativo que puede procesar texto, imágenes, audio y video simultáneamente desde su arquitectura base.
                    </p>
                    <div class="mt-4 text-center">
                        <span class="inline-block bg-indigo-100 text-indigo-800 text-xs px-2 py-1 rounded-full mr-2">Parámetros variables</span>
                        <span class="inline-block bg-indigo-100 text-indigo-800 text-xs px-2 py-1 rounded-full">Multimodal</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- Proceso de Entrenamiento -->
        <section class="mb-16">
            <h2 class="slide-content-title">Proceso de Entrenamiento de LLMs</h2>
            <div class="accent-line"></div>
            
            <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
                <!-- Pre-entrenamiento -->
                <div class="icon-card text-center">
                    <div class="w-16 h-16 bg-blue-100 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-book text-blue-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-4">1. Pre-entrenamiento</h3>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Entrenamiento no supervisado en billones de tokens de texto de internet para aprender patrones de lenguaje general.
                    </p>
                </div>

                <!-- Fine-tuning -->
                <div class="icon-card text-center">
                    <div class="w-16 h-16 bg-green-100 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-cogs text-green-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-4">2. Fine-tuning Supervisado</h3>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Ajuste con ejemplos de alta calidad de conversaciones humano-IA para mejorar la capacidad de seguir instrucciones.
                    </p>
                </div>

                <!-- RLHF -->
                <div class="icon-card text-center">
                    <div class="w-16 h-16 bg-purple-100 rounded-full flex items-center justify-center mx-auto mb-4">
                        <i class="fas fa-heart text-purple-600 text-xl"></i>
                    </div>
                    <h3 class="text-xl font-bold mb-4">3. RLHF</h3>
                    <p class="text-gray-600 text-sm leading-relaxed">
                        Aprendizaje por Refuerzo desde Feedback Humano para alinear las respuestas con las preferencias humanas.
                    </p>
                </div>
            </div>
        </section>

        <!-- Capacidades Emergentes -->
        <section class="mb-16">
            <div class="content-card card-hover">
                <h2 class="text-2xl font-bold mb-8 text-center">Capacidades Emergentes en LLMs</h2>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div>
                        <h3 class="text-xl font-bold mb-4 text-primary flex items-center">
                            <i class="fas fa-brain mr-3"></i>
                            Razonamiento en Cadena de Pensamiento
                        </h3>
                        <p class="text-gray-600 mb-4 leading-relaxed">
                            Los LLMs desarrollan la capacidad de descomponer problemas complejos en pasos lógicos intermedios, mejorando significativamente en matemáticas y razonamiento.
                        </p>
                        <div class="highlight-box">
                            <p class="text-sm text-red-800">
                                <strong>Ejemplo:</strong> "Pensemos paso a paso..." desencadena razonamiento explícito
                            </p>
                        </div>
                    </div>
                    <div>
                        <h3 class="text-xl font-bold mb-4 text-primary flex items-center">
                            <i class="fas fa-lightbulb mr-3"></i>
                            Aprendizaje en Contexto
                        </h3>
                        <p class="text-gray-600 mb-4 leading-relaxed">
                            Capacidad de aprender nuevas tareas a partir de solo unos pocos ejemplos en el prompt, sin necesidad de reentrenamiento.
                        </p>
                        <div class="highlight-box">
                            <p class="text-sm text-red-800">
                                <strong>Few-shot learning:</strong> 2-5 ejemplos pueden enseñar patrones complejos
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Limitaciones y Desafíos -->
        <section class="mb-16">
            <div class="content-card card-hover bg-white">
                <h2 class="text-2xl font-bold mb-8 text-center text-gray-800">Limitaciones y Desafíos Actuales</h2>
                <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
                    <div class="text-center bg-red-50 p-6 rounded-lg border-l-4 border-red-500">
                        <div class="bg-red-100 rounded-full w-16 h-16 flex items-center justify-center mx-auto mb-3">
                            <i class="fas fa-exclamation-triangle text-xl text-red-600"></i>
                        </div>
                        <h3 class="font-bold mb-2 text-gray-800">Alucinaciones</h3>
                        <p class="text-sm text-gray-700">Generación de información falsa o inexistente presentada con confianza</p>
                    </div>
                    <div class="text-center bg-orange-50 p-6 rounded-lg border-l-4 border-orange-500">
                        <div class="bg-orange-100 rounded-full w-16 h-16 flex items-center justify-center mx-auto mb-3">
                            <i class="fas fa-balance-scale text-xl text-orange-600"></i>
                        </div>
                        <h3 class="font-bold mb-2 text-gray-800">Sesgo de Datos</h3>
                        <p class="text-sm text-gray-700">Reproducción de sesgos presentes en los datos de entrenamiento</p>
                    </div>
                    <div class="text-center bg-yellow-50 p-6 rounded-lg border-l-4 border-yellow-500">
                        <div class="bg-yellow-100 rounded-full w-16 h-16 flex items-center justify-center mx-auto mb-3">
                            <i class="fas fa-question-circle text-xl text-yellow-600"></i>
                        </div>
                        <h3 class="font-bold mb-2 text-gray-800">Falta de Transparencia</h3>
                        <p class="text-sm text-gray-700">Dificultad para explicar cómo llegan a sus conclusiones (problema de la caja negra)</p>
                    </div>
                    <div class="text-center bg-blue-50 p-6 rounded-lg border-l-4 border-blue-500">
                        <div class="bg-blue-100 rounded-full w-16 h-16 flex items-center justify-center mx-auto mb-3">
                            <i class="fas fa-server text-xl text-blue-600"></i>
                        </div>
                        <h3 class="font-bold mb-2 text-gray-800">Costo Computacional</h3>
                        <p class="text-sm text-gray-700">Requieren recursos masivos para entrenamiento e inferencia</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Aplicaciones Educativas -->
        <section class="mb-16">
            <h2 class="slide-content-title">Aplicaciones en Educación</h2>
            <div class="accent-line"></div>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-primary/10 p-3 rounded-full mr-4">
                            <i class="fas fa-user-graduate text-primary text-xl"></i>
                        </div>
                        <h3 class="font-bold text-lg">Tutoría Personalizada</h3>
                    </div>
                    <p class="text-gray-600 leading-relaxed">LLMs pueden adaptar explicaciones al nivel y estilo de aprendizaje de cada estudiante, proporcionando retroalimentación inmediata.</p>
                </div>
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-secondary/10 p-3 rounded-full mr-4">
                            <i class="fas fa-file-alt text-secondary text-xl"></i>
                        </div>
                        <h3 class="font-bold text-lg">Generación de Contenido</h3>
                    </div>
                    <p class="text-gray-600 leading-relaxed">Creación automática de ejercicios, exámenes, y materiales educativos adaptados a objetivos específicos de aprendizaje.</p>
                </div>
                <div class="icon-card">
                    <div class="flex items-center mb-4">
                        <div class="bg-accent/10 p-3 rounded-full mr-4">
                            <i class="fas fa-microscope text-accent text-xl"></i>
                        </div>
                        <h3 class="font-bold text-lg">Asistencia en Investigación</h3>
                    </div>
                    <p class="text-gray-600 leading-relaxed">Ayuda en búsqueda de información, síntesis de fuentes, y escritura académica con verificación de hechos.</p>
                </div>
            </div>
        </section>

        <!-- Future Trends -->
        <section class="mb-16">
            <div class="content-card card-hover bg-white">
                <h2 class="text-2xl font-bold mb-8 text-center text-gray-800">Tendencias Futuras</h2>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div class="bg-indigo-50 p-6 rounded-lg border-l-4 border-indigo-500">
                        <h3 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                            <i class="fas fa-images mr-3 text-indigo-600"></i>
                            Modelos Multimodales
                        </h3>
                        <p class="text-gray-700 leading-relaxed">
                            Integración nativa de texto, imagen, audio y video en una sola arquitectura para comprensión holística.
                        </p>
                    </div>
                    <div class="bg-green-50 p-6 rounded-lg border-l-4 border-green-500">
                        <h3 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                            <i class="fas fa-calculator mr-3 text-green-600"></i>
                            Reasoning Engines
                        </h3>
                        <p class="text-gray-700 leading-relaxed">
                            Modelos especializados en razonamiento lógico y matemático que pueden verificar sus propias respuestas.
                        </p>
                    </div>
                    <div class="bg-blue-50 p-6 rounded-lg border-l-4 border-blue-500">
                        <h3 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                            <i class="fas fa-tachometer-alt mr-3 text-blue-600"></i>
                            Modelos Más Eficientes
                        </h3>
                        <p class="text-gray-700 leading-relaxed">
                            Arquitecturas que mantienen capacidades con menos parámetros, reduciendo costos y democratizando el acceso.
                        </p>
                    </div>
                    <div class="bg-purple-50 p-6 rounded-lg border-l-4 border-purple-500">
                        <h3 class="text-xl font-bold mb-4 flex items-center text-gray-800">
                            <i class="fas fa-robot mr-3 text-purple-600"></i>
                            Agentes Autónomos
                        </h3>
                        <p class="text-gray-700 leading-relaxed">
                            LLMs que pueden planificar, ejecutar tareas complejas y aprender de la experiencia de forma autónoma.
                        </p>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer con navegación -->
    <footer class="bg-dark text-white py-12 px-4 sm:px-6 lg:px-8 mt-20">
        <div class="max-w-7xl mx-auto text-center">
            <div class="mb-8">
                <a href="modulo2.html" class="nav-button">
                    <i class="fas fa-arrow-left"></i>
                    Volver al Módulo 2
                </a>
            </div>
            <div class="border-t border-gray-700 pt-8 text-center text-gray-400 text-sm">
                <p>Diplomado: Herramientas de Inteligencia Artificial para Educadores Innovadores.</p>
            </div>
        </div>
    </footer>
</body>
</html>
